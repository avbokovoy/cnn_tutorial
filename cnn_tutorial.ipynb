{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключаем необходимые библиотеки\n",
    "import os                                   # Модуль для работы с операционной системой\n",
    "    \n",
    "import torch                                # Основной модуль PyTorch \n",
    "import torch.nn as nn                       # Модуль PyTorch для работы с нейронными сетями\n",
    "import torch.utils.data as Data             # Модуль для работы с Dataset'ами\n",
    "\n",
    "import torchvision                          # Основной модуль PyTorch для работы с изображениями\n",
    "import torchvision.transforms as transforms # Модуль для преобразования изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1c880d6230>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)                        # Настраиваем генератор случайных чисел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель нейронной сети:\n",
    "class CNN( nn.Module ):\n",
    "    def __init__( self ):\n",
    "        super( CNN, self ).__init__()\n",
    "        self.conv1 = nn.Sequential(                   # Первый слой. На вход подается изображение с 3 каналами (RGB), размером 32х32 \n",
    "                                                      # или (3, 32, 32) \n",
    "            nn.Conv2d(                                # Функиця свертки.  \n",
    "                in_channels  = 3                      # Количество входящих каналов - 3, равно количеству каналов изображения\n",
    "              , out_channels = 16                     # Количество исходящих каналов - 16\n",
    "              , kernel_size  = 5                      # Размер ядра свертки - 5\n",
    "              , stride       = 1                      # Смещение при свертке по горизонтали и вертикали - 1\n",
    "              , padding      = 2                      # Количество дополнительных столбцов и строк, заполненных нулями, у границ изображения\n",
    "            )\n",
    "            , nn.ReLU()                               # Функция активации\n",
    "            , nn.MaxPool2d( kernel_size = 2 )         # Функция субдискретизации с размером ядра - 2\n",
    "        )                                             # На выходе - изображение ( 16, 16, 16 )  \n",
    "        \n",
    "        self.conv2 = nn.Sequential(                   # Второй слой. На входе изображение ( 16, 16, 16 )\n",
    "              nn.Conv2d( 16, 32, 5, 1, 2 )            # Функция свертки.\n",
    "            , nn.ReLU()                               # Функция активации.\n",
    "            , nn.MaxPool2d( 2 )                       # Функция субдискретизации. \n",
    "         )                                            # На выходе - изображение ( 32, 8, 8 )\n",
    "        \n",
    "        self.out = nn.Linear( 32 * 8 * 8, 10 )        # Персептрон. Первый аргумент - количество входящих пикселей.\n",
    "                                                      # Второй - размер полносвязного слоя (равен количеству классов, \n",
    "                                                      # на которые поделены изображения).\n",
    "\n",
    "    def forward( self, x ):                           # Переопределяем метод forward класса nn.Module\n",
    "        x = self.conv1(x)                             # Применяем первый слой.\n",
    "        x = self.conv2(x)                             # Применяем второй слой.\n",
    "        x = x.view( x.size(0), -1 )                   # 'Сплющиваем' до одномерного массива  \n",
    "        x = self.out(x)                               # Формируем перцептрон\n",
    "        return x                                      # Возвращаем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()                                           # Создаем объект типа CNN\n",
    "print(cnn)                                            # Выводим на экран архитектуру сети в текстовом виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(            # Скачиваем dataset Cifar10 https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "      root      = './cifar10/'                        # Папка, в которую будут скачаны изображения, разметка и т.д.\n",
    "    , train     = True                                # Если True, данные будут созданы из тренировочного dataset'a\n",
    "                                                      # False - из тестировочного.\n",
    "    , transform = torchvision.transforms.ToTensor()   # Функция преобразования исходных данных в PyTorch Tensor.\n",
    "    , download  = True                                # Если True, то данные будут скачаны из удаленного репозитория, \n",
    "                                                      # False - будут использованы локальные данные\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(                       # Создаем объект типа Data.DataLoader\n",
    "      dataset    = train_data                         # Входящий dataset'a - train_data\n",
    "    , batch_size = 50                                 # Сколько изображений войдут в партию\n",
    "    , shuffle    = True                               # Нужно ли перемешивать изображения в dataset'e.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявляем оптимизатор (в конкретном случае - алгоритм Адама).\n",
    "# На входе - параметры сети и скорость обучения (learning rate)\n",
    "optimizer = torch.optim.Adam( cnn.parameters(), lr = 0.001 )  \n",
    "\n",
    "# Объявляем функцию потерь (в конкретном случае - функция кросс-энтропии)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,     0] loss: 1.212199\n",
      "[0,     1] loss: 1.194162\n",
      "[0,     2] loss: 1.321516\n",
      "[0,     3] loss: 1.038327\n",
      "[0,     4] loss: 1.359172\n",
      "[0,     5] loss: 1.126495\n",
      "[0,     6] loss: 1.227483\n",
      "[0,     7] loss: 1.113353\n",
      "[0,     8] loss: 1.431833\n",
      "[0,     9] loss: 1.196281\n",
      "[0,    10] loss: 1.222817\n",
      "[0,    11] loss: 1.250081\n",
      "[0,    12] loss: 1.002198\n",
      "[0,    13] loss: 1.357305\n",
      "[0,    14] loss: 1.409003\n",
      "[0,    15] loss: 1.212957\n",
      "[0,    16] loss: 1.256971\n",
      "[0,    17] loss: 1.440481\n",
      "[0,    18] loss: 1.188137\n",
      "[0,    19] loss: 1.008883\n",
      "[0,    20] loss: 1.317479\n",
      "[0,    21] loss: 1.278761\n",
      "[0,    22] loss: 1.408422\n",
      "[0,    23] loss: 1.485556\n",
      "[0,    24] loss: 1.181137\n",
      "[0,    25] loss: 1.431250\n",
      "[0,    26] loss: 1.215267\n",
      "[0,    27] loss: 0.997111\n",
      "[0,    28] loss: 1.388735\n",
      "[0,    29] loss: 1.394551\n",
      "[0,    30] loss: 1.576741\n",
      "[0,    31] loss: 1.264168\n",
      "[0,    32] loss: 1.315991\n",
      "[0,    33] loss: 1.228331\n",
      "[0,    34] loss: 1.131516\n",
      "[0,    35] loss: 1.249092\n",
      "[0,    36] loss: 1.224394\n",
      "[0,    37] loss: 1.303949\n",
      "[0,    38] loss: 1.068414\n",
      "[0,    39] loss: 1.270576\n",
      "[0,    40] loss: 1.347274\n",
      "[0,    41] loss: 1.343797\n",
      "[0,    42] loss: 1.248350\n",
      "[0,    43] loss: 1.068905\n",
      "[0,    44] loss: 1.133569\n",
      "[0,    45] loss: 1.497535\n",
      "[0,    46] loss: 1.295042\n",
      "[0,    47] loss: 1.383063\n",
      "[0,    48] loss: 1.263877\n",
      "[0,    49] loss: 1.475166\n",
      "[0,    50] loss: 1.238545\n",
      "[0,    51] loss: 1.429681\n",
      "[0,    52] loss: 1.483051\n",
      "[0,    53] loss: 1.415283\n",
      "[0,    54] loss: 1.334371\n",
      "[0,    55] loss: 1.353123\n",
      "[0,    56] loss: 1.592984\n",
      "[0,    57] loss: 1.354327\n",
      "[0,    58] loss: 1.452582\n",
      "[0,    59] loss: 1.313256\n",
      "[0,    60] loss: 1.417856\n",
      "[0,    61] loss: 1.615555\n",
      "[0,    62] loss: 1.170137\n",
      "[0,    63] loss: 1.094424\n",
      "[0,    64] loss: 1.429054\n",
      "[0,    65] loss: 1.201432\n",
      "[0,    66] loss: 1.232333\n",
      "[0,    67] loss: 1.300100\n",
      "[0,    68] loss: 1.221036\n",
      "[0,    69] loss: 1.392332\n",
      "[0,    70] loss: 1.161287\n",
      "[0,    71] loss: 1.418599\n",
      "[0,    72] loss: 1.240446\n",
      "[0,    73] loss: 1.489793\n",
      "[0,    74] loss: 1.368464\n",
      "[0,    75] loss: 1.229167\n",
      "[0,    76] loss: 1.098587\n",
      "[0,    77] loss: 1.040563\n",
      "[0,    78] loss: 1.104264\n",
      "[0,    79] loss: 1.127455\n",
      "[0,    80] loss: 1.174261\n",
      "[0,    81] loss: 1.189398\n",
      "[0,    82] loss: 1.407162\n",
      "[0,    83] loss: 1.299204\n",
      "[0,    84] loss: 1.077541\n",
      "[0,    85] loss: 1.356595\n",
      "[0,    86] loss: 1.361468\n",
      "[0,    87] loss: 1.423621\n",
      "[0,    88] loss: 1.317650\n",
      "[0,    89] loss: 1.031589\n",
      "[0,    90] loss: 1.204667\n",
      "[0,    91] loss: 1.259213\n",
      "[0,    92] loss: 1.266485\n",
      "[0,    93] loss: 1.081157\n",
      "[0,    94] loss: 1.006703\n",
      "[0,    95] loss: 1.138693\n",
      "[0,    96] loss: 1.116674\n",
      "[0,    97] loss: 1.178363\n",
      "[0,    98] loss: 1.467449\n",
      "[0,    99] loss: 1.233052\n",
      "[0,   100] loss: 1.438408\n",
      "[0,   101] loss: 1.328987\n",
      "[0,   102] loss: 1.304307\n",
      "[0,   103] loss: 1.364620\n",
      "[0,   104] loss: 1.228668\n",
      "[0,   105] loss: 1.090217\n",
      "[0,   106] loss: 1.264418\n",
      "[0,   107] loss: 1.187638\n",
      "[0,   108] loss: 1.433738\n",
      "[0,   109] loss: 1.088520\n",
      "[0,   110] loss: 1.187990\n",
      "[0,   111] loss: 1.250750\n",
      "[0,   112] loss: 1.049761\n",
      "[0,   113] loss: 1.182872\n",
      "[0,   114] loss: 1.320540\n",
      "[0,   115] loss: 1.020885\n",
      "[0,   116] loss: 0.984269\n",
      "[0,   117] loss: 1.321638\n",
      "[0,   118] loss: 1.124856\n",
      "[0,   119] loss: 1.480549\n",
      "[0,   120] loss: 1.136394\n",
      "[0,   121] loss: 1.308686\n",
      "[0,   122] loss: 1.344360\n",
      "[0,   123] loss: 1.051033\n",
      "[0,   124] loss: 1.296951\n",
      "[0,   125] loss: 1.518334\n",
      "[0,   126] loss: 1.493201\n",
      "[0,   127] loss: 1.333156\n",
      "[0,   128] loss: 1.486377\n",
      "[0,   129] loss: 1.167904\n",
      "[0,   130] loss: 1.103941\n",
      "[0,   131] loss: 1.403569\n",
      "[0,   132] loss: 1.317042\n",
      "[0,   133] loss: 1.401844\n",
      "[0,   134] loss: 1.228614\n",
      "[0,   135] loss: 0.980660\n",
      "[0,   136] loss: 1.344115\n",
      "[0,   137] loss: 1.271253\n",
      "[0,   138] loss: 1.416970\n",
      "[0,   139] loss: 1.131748\n",
      "[0,   140] loss: 1.180717\n",
      "[0,   141] loss: 1.297589\n",
      "[0,   142] loss: 1.255893\n",
      "[0,   143] loss: 1.606199\n",
      "[0,   144] loss: 1.255998\n",
      "[0,   145] loss: 1.114663\n",
      "[0,   146] loss: 1.145570\n",
      "[0,   147] loss: 1.044187\n",
      "[0,   148] loss: 1.409639\n",
      "[0,   149] loss: 1.300309\n",
      "[0,   150] loss: 0.998750\n",
      "[0,   151] loss: 1.178431\n",
      "[0,   152] loss: 1.199429\n",
      "[0,   153] loss: 1.207484\n",
      "[0,   154] loss: 1.278905\n",
      "[0,   155] loss: 1.028536\n",
      "[0,   156] loss: 1.253688\n",
      "[0,   157] loss: 1.094541\n",
      "[0,   158] loss: 1.413901\n",
      "[0,   159] loss: 1.344251\n",
      "[0,   160] loss: 1.254058\n",
      "[0,   161] loss: 0.982182\n",
      "[0,   162] loss: 1.323349\n",
      "[0,   163] loss: 1.687849\n",
      "[0,   164] loss: 1.178153\n",
      "[0,   165] loss: 1.268065\n",
      "[0,   166] loss: 1.317871\n",
      "[0,   167] loss: 1.193561\n",
      "[0,   168] loss: 1.229477\n",
      "[0,   169] loss: 1.197769\n",
      "[0,   170] loss: 1.309202\n",
      "[0,   171] loss: 1.177387\n",
      "[0,   172] loss: 1.263545\n",
      "[0,   173] loss: 1.335592\n",
      "[0,   174] loss: 1.318240\n",
      "[0,   175] loss: 1.439786\n",
      "[0,   176] loss: 1.169470\n",
      "[0,   177] loss: 1.177902\n",
      "[0,   178] loss: 1.126974\n",
      "[0,   179] loss: 1.492575\n",
      "[0,   180] loss: 1.542870\n",
      "[0,   181] loss: 1.250201\n",
      "[0,   182] loss: 1.261998\n",
      "[0,   183] loss: 1.201610\n",
      "[0,   184] loss: 0.981624\n",
      "[0,   185] loss: 1.433934\n",
      "[0,   186] loss: 1.473373\n",
      "[0,   187] loss: 1.242043\n",
      "[0,   188] loss: 1.445839\n",
      "[0,   189] loss: 1.561490\n",
      "[0,   190] loss: 1.255723\n",
      "[0,   191] loss: 1.719666\n",
      "[0,   192] loss: 1.203966\n",
      "[0,   193] loss: 1.120561\n",
      "[0,   194] loss: 1.226993\n",
      "[0,   195] loss: 1.153588\n",
      "[0,   196] loss: 1.186712\n",
      "[0,   197] loss: 1.307047\n",
      "[0,   198] loss: 1.074237\n",
      "[0,   199] loss: 0.941645\n",
      "[0,   200] loss: 1.436484\n",
      "[0,   201] loss: 0.910281\n",
      "[0,   202] loss: 1.377198\n",
      "[0,   203] loss: 1.101540\n",
      "[0,   204] loss: 1.564486\n",
      "[0,   205] loss: 1.151130\n",
      "[0,   206] loss: 1.194023\n",
      "[0,   207] loss: 1.387981\n",
      "[0,   208] loss: 1.245430\n",
      "[0,   209] loss: 1.236262\n",
      "[0,   210] loss: 1.208981\n",
      "[0,   211] loss: 1.321941\n",
      "[0,   212] loss: 0.928395\n",
      "[0,   213] loss: 0.982320\n",
      "[0,   214] loss: 1.158459\n",
      "[0,   215] loss: 1.444918\n",
      "[0,   216] loss: 1.182160\n",
      "[0,   217] loss: 1.196488\n",
      "[0,   218] loss: 0.768949\n",
      "[0,   219] loss: 1.274191\n",
      "[0,   220] loss: 1.063839\n",
      "[0,   221] loss: 1.143988\n",
      "[0,   222] loss: 1.453991\n",
      "[0,   223] loss: 1.130178\n",
      "[0,   224] loss: 1.211716\n",
      "[0,   225] loss: 1.366229\n",
      "[0,   226] loss: 1.194745\n",
      "[0,   227] loss: 1.035947\n",
      "[0,   228] loss: 1.142366\n",
      "[0,   229] loss: 1.021197\n",
      "[0,   230] loss: 1.290056\n",
      "[0,   231] loss: 1.209510\n",
      "[0,   232] loss: 1.373801\n",
      "[0,   233] loss: 1.317941\n",
      "[0,   234] loss: 1.218228\n",
      "[0,   235] loss: 1.455165\n",
      "[0,   236] loss: 1.254063\n",
      "[0,   237] loss: 1.212097\n",
      "[0,   238] loss: 1.291280\n",
      "[0,   239] loss: 1.090508\n",
      "[0,   240] loss: 1.265876\n",
      "[0,   241] loss: 1.302903\n",
      "[0,   242] loss: 1.494873\n",
      "[0,   243] loss: 1.048684\n",
      "[0,   244] loss: 1.535877\n",
      "[0,   245] loss: 1.220107\n",
      "[0,   246] loss: 1.135732\n",
      "[0,   247] loss: 1.274074\n",
      "[0,   248] loss: 1.415324\n",
      "[0,   249] loss: 1.481964\n",
      "[0,   250] loss: 1.192841\n",
      "[0,   251] loss: 1.400995\n",
      "[0,   252] loss: 0.934962\n",
      "[0,   253] loss: 1.379370\n",
      "[0,   254] loss: 1.177230\n",
      "[0,   255] loss: 1.269273\n",
      "[0,   256] loss: 0.794976\n",
      "[0,   257] loss: 1.337051\n",
      "[0,   258] loss: 1.312510\n",
      "[0,   259] loss: 1.286476\n",
      "[0,   260] loss: 1.119551\n",
      "[0,   261] loss: 1.146565\n",
      "[0,   262] loss: 1.227411\n",
      "[0,   263] loss: 1.076694\n",
      "[0,   264] loss: 1.250870\n",
      "[0,   265] loss: 1.215966\n",
      "[0,   266] loss: 1.119858\n",
      "[0,   267] loss: 1.275100\n",
      "[0,   268] loss: 1.215010\n",
      "[0,   269] loss: 1.179255\n",
      "[0,   270] loss: 1.131578\n",
      "[0,   271] loss: 1.311777\n",
      "[0,   272] loss: 1.123633\n",
      "[0,   273] loss: 1.465306\n",
      "[0,   274] loss: 1.046016\n",
      "[0,   275] loss: 1.098232\n",
      "[0,   276] loss: 1.169979\n",
      "[0,   277] loss: 1.158057\n",
      "[0,   278] loss: 1.396704\n",
      "[0,   279] loss: 1.249086\n",
      "[0,   280] loss: 1.098959\n",
      "[0,   281] loss: 0.990629\n",
      "[0,   282] loss: 1.067971\n",
      "[0,   283] loss: 1.078109\n",
      "[0,   284] loss: 1.495390\n",
      "[0,   285] loss: 1.267393\n",
      "[0,   286] loss: 1.532037\n",
      "[0,   287] loss: 1.012100\n",
      "[0,   288] loss: 1.149172\n",
      "[0,   289] loss: 1.323271\n",
      "[0,   290] loss: 1.379688\n",
      "[0,   291] loss: 1.049682\n",
      "[0,   292] loss: 1.169932\n",
      "[0,   293] loss: 1.111202\n",
      "[0,   294] loss: 1.243655\n",
      "[0,   295] loss: 1.270916\n",
      "[0,   296] loss: 1.304880\n",
      "[0,   297] loss: 0.969009\n",
      "[0,   298] loss: 1.092483\n",
      "[0,   299] loss: 1.124386\n",
      "[0,   300] loss: 1.090298\n",
      "[0,   301] loss: 1.214596\n",
      "[0,   302] loss: 1.493615\n",
      "[0,   303] loss: 0.994043\n",
      "[0,   304] loss: 1.134924\n",
      "[0,   305] loss: 1.283085\n",
      "[0,   306] loss: 1.164086\n",
      "[0,   307] loss: 1.143243\n",
      "[0,   308] loss: 1.085959\n",
      "[0,   309] loss: 1.363975\n",
      "[0,   310] loss: 1.337301\n",
      "[0,   311] loss: 1.004242\n",
      "[0,   312] loss: 1.197546\n",
      "[0,   313] loss: 1.257110\n",
      "[0,   314] loss: 1.061862\n",
      "[0,   315] loss: 1.124625\n",
      "[0,   316] loss: 1.084917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,   317] loss: 1.018742\n",
      "[0,   318] loss: 1.263996\n",
      "[0,   319] loss: 0.921696\n",
      "[0,   320] loss: 1.092843\n",
      "[0,   321] loss: 1.362373\n",
      "[0,   322] loss: 0.964975\n",
      "[0,   323] loss: 0.826981\n",
      "[0,   324] loss: 1.139465\n",
      "[0,   325] loss: 1.241177\n",
      "[0,   326] loss: 1.215936\n",
      "[0,   327] loss: 1.256716\n",
      "[0,   328] loss: 1.300020\n",
      "[0,   329] loss: 0.978915\n",
      "[0,   330] loss: 1.197294\n",
      "[0,   331] loss: 1.094666\n",
      "[0,   332] loss: 1.378112\n",
      "[0,   333] loss: 1.229049\n",
      "[0,   334] loss: 1.473172\n",
      "[0,   335] loss: 1.088793\n",
      "[0,   336] loss: 1.434671\n",
      "[0,   337] loss: 1.180688\n",
      "[0,   338] loss: 1.181600\n",
      "[0,   339] loss: 0.819961\n",
      "[0,   340] loss: 1.122605\n",
      "[0,   341] loss: 1.238264\n",
      "[0,   342] loss: 1.274619\n",
      "[0,   343] loss: 1.157172\n",
      "[0,   344] loss: 1.261173\n",
      "[0,   345] loss: 1.074426\n",
      "[0,   346] loss: 1.127328\n",
      "[0,   347] loss: 1.111158\n",
      "[0,   348] loss: 1.168233\n",
      "[0,   349] loss: 1.347462\n",
      "[0,   350] loss: 1.248440\n",
      "[0,   351] loss: 1.164575\n",
      "[0,   352] loss: 1.236675\n",
      "[0,   353] loss: 1.218044\n",
      "[0,   354] loss: 0.951263\n",
      "[0,   355] loss: 1.165084\n",
      "[0,   356] loss: 1.228362\n",
      "[0,   357] loss: 1.241637\n",
      "[0,   358] loss: 1.268144\n",
      "[0,   359] loss: 1.344707\n",
      "[0,   360] loss: 1.151166\n",
      "[0,   361] loss: 1.130283\n",
      "[0,   362] loss: 1.017641\n",
      "[0,   363] loss: 1.202843\n",
      "[0,   364] loss: 1.053279\n",
      "[0,   365] loss: 1.532177\n",
      "[0,   366] loss: 1.395354\n",
      "[0,   367] loss: 1.112532\n",
      "[0,   368] loss: 1.161155\n",
      "[0,   369] loss: 1.053573\n",
      "[0,   370] loss: 1.470996\n",
      "[0,   371] loss: 1.075856\n",
      "[0,   372] loss: 1.321588\n",
      "[0,   373] loss: 1.285207\n",
      "[0,   374] loss: 0.979980\n",
      "[0,   375] loss: 1.431017\n",
      "[0,   376] loss: 1.174241\n",
      "[0,   377] loss: 1.129261\n",
      "[0,   378] loss: 1.400609\n",
      "[0,   379] loss: 1.156158\n",
      "[0,   380] loss: 1.137281\n",
      "[0,   381] loss: 1.195598\n",
      "[0,   382] loss: 1.279809\n",
      "[0,   383] loss: 1.305060\n",
      "[0,   384] loss: 1.121465\n",
      "[0,   385] loss: 1.060933\n",
      "[0,   386] loss: 1.249552\n",
      "[0,   387] loss: 1.047616\n",
      "[0,   388] loss: 1.101449\n",
      "[0,   389] loss: 1.220241\n",
      "[0,   390] loss: 1.066956\n",
      "[0,   391] loss: 1.239625\n",
      "[0,   392] loss: 1.372107\n",
      "[0,   393] loss: 1.015018\n",
      "[0,   394] loss: 1.022929\n",
      "[0,   395] loss: 1.246907\n",
      "[0,   396] loss: 1.479195\n",
      "[0,   397] loss: 1.092514\n",
      "[0,   398] loss: 1.356883\n",
      "[0,   399] loss: 1.033239\n",
      "[0,   400] loss: 1.094120\n",
      "[0,   401] loss: 1.263399\n",
      "[0,   402] loss: 1.252760\n",
      "[0,   403] loss: 1.119331\n",
      "[0,   404] loss: 1.116851\n",
      "[0,   405] loss: 0.942884\n",
      "[0,   406] loss: 0.825012\n",
      "[0,   407] loss: 1.105958\n",
      "[0,   408] loss: 1.120264\n",
      "[0,   409] loss: 1.172600\n",
      "[0,   410] loss: 1.272211\n",
      "[0,   411] loss: 1.122893\n",
      "[0,   412] loss: 1.125610\n",
      "[0,   413] loss: 1.096029\n",
      "[0,   414] loss: 1.154195\n",
      "[0,   415] loss: 1.315824\n",
      "[0,   416] loss: 1.155822\n",
      "[0,   417] loss: 1.152117\n",
      "[0,   418] loss: 1.225417\n",
      "[0,   419] loss: 1.321241\n",
      "[0,   420] loss: 1.105587\n",
      "[0,   421] loss: 1.055956\n",
      "[0,   422] loss: 1.374606\n",
      "[0,   423] loss: 1.175777\n",
      "[0,   424] loss: 1.341413\n",
      "[0,   425] loss: 1.202885\n",
      "[0,   426] loss: 1.270310\n",
      "[0,   427] loss: 1.263499\n",
      "[0,   428] loss: 1.525786\n",
      "[0,   429] loss: 1.095229\n",
      "[0,   430] loss: 1.113026\n",
      "[0,   431] loss: 1.232587\n",
      "[0,   432] loss: 1.444320\n",
      "[0,   433] loss: 1.001594\n",
      "[0,   434] loss: 1.061226\n",
      "[0,   435] loss: 1.253308\n",
      "[0,   436] loss: 1.276786\n",
      "[0,   437] loss: 1.094799\n",
      "[0,   438] loss: 1.038753\n",
      "[0,   439] loss: 1.214426\n",
      "[0,   440] loss: 1.400301\n",
      "[0,   441] loss: 1.217640\n",
      "[0,   442] loss: 1.224855\n",
      "[0,   443] loss: 0.871425\n",
      "[0,   444] loss: 1.123686\n",
      "[0,   445] loss: 1.429984\n",
      "[0,   446] loss: 1.213778\n",
      "[0,   447] loss: 1.232778\n",
      "[0,   448] loss: 1.044940\n",
      "[0,   449] loss: 1.425637\n",
      "[0,   450] loss: 0.906684\n",
      "[0,   451] loss: 1.020704\n",
      "[0,   452] loss: 1.368225\n",
      "[0,   453] loss: 1.067304\n",
      "[0,   454] loss: 1.322283\n",
      "[0,   455] loss: 1.098159\n",
      "[0,   456] loss: 1.043473\n",
      "[0,   457] loss: 1.013830\n",
      "[0,   458] loss: 1.212332\n",
      "[0,   459] loss: 0.908915\n",
      "[0,   460] loss: 1.290352\n",
      "[0,   461] loss: 1.163489\n",
      "[0,   462] loss: 0.991245\n",
      "[0,   463] loss: 1.085009\n",
      "[0,   464] loss: 1.162083\n",
      "[0,   465] loss: 1.338385\n",
      "[0,   466] loss: 1.154561\n",
      "[0,   467] loss: 1.376389\n",
      "[0,   468] loss: 1.210392\n",
      "[0,   469] loss: 1.319800\n",
      "[0,   470] loss: 1.180615\n",
      "[0,   471] loss: 1.301119\n",
      "[0,   472] loss: 1.244211\n",
      "[0,   473] loss: 1.230291\n",
      "[0,   474] loss: 1.316872\n",
      "[0,   475] loss: 1.109340\n",
      "[0,   476] loss: 1.215503\n",
      "[0,   477] loss: 1.303039\n",
      "[0,   478] loss: 1.025054\n",
      "[0,   479] loss: 1.139070\n",
      "[0,   480] loss: 1.216980\n",
      "[0,   481] loss: 1.335659\n",
      "[0,   482] loss: 1.123278\n",
      "[0,   483] loss: 1.243286\n",
      "[0,   484] loss: 1.146320\n",
      "[0,   485] loss: 1.138521\n",
      "[0,   486] loss: 1.438817\n",
      "[0,   487] loss: 1.581487\n",
      "[0,   488] loss: 1.226482\n",
      "[0,   489] loss: 1.354855\n",
      "[0,   490] loss: 1.059117\n",
      "[0,   491] loss: 1.387599\n",
      "[0,   492] loss: 1.406669\n",
      "[0,   493] loss: 1.399341\n",
      "[0,   494] loss: 0.997545\n",
      "[0,   495] loss: 1.130862\n",
      "[0,   496] loss: 1.352683\n",
      "[0,   497] loss: 0.942014\n",
      "[0,   498] loss: 1.138373\n",
      "[0,   499] loss: 1.155522\n",
      "[0,   500] loss: 1.329588\n",
      "[0,   501] loss: 1.104800\n",
      "[0,   502] loss: 1.384094\n",
      "[0,   503] loss: 1.416150\n",
      "[0,   504] loss: 1.254440\n",
      "[0,   505] loss: 1.431783\n",
      "[0,   506] loss: 1.127792\n",
      "[0,   507] loss: 1.013956\n",
      "[0,   508] loss: 1.074638\n",
      "[0,   509] loss: 1.290716\n",
      "[0,   510] loss: 0.884372\n",
      "[0,   511] loss: 1.536708\n",
      "[0,   512] loss: 1.515939\n",
      "[0,   513] loss: 1.352744\n",
      "[0,   514] loss: 1.181450\n",
      "[0,   515] loss: 1.369590\n",
      "[0,   516] loss: 1.590667\n",
      "[0,   517] loss: 0.937918\n",
      "[0,   518] loss: 1.195756\n",
      "[0,   519] loss: 0.964290\n",
      "[0,   520] loss: 1.256503\n",
      "[0,   521] loss: 1.012003\n",
      "[0,   522] loss: 1.336431\n",
      "[0,   523] loss: 1.263380\n",
      "[0,   524] loss: 1.122829\n",
      "[0,   525] loss: 1.132068\n",
      "[0,   526] loss: 1.247010\n",
      "[0,   527] loss: 1.374484\n",
      "[0,   528] loss: 1.142773\n",
      "[0,   529] loss: 1.063532\n",
      "[0,   530] loss: 1.146059\n",
      "[0,   531] loss: 1.387636\n",
      "[0,   532] loss: 0.699276\n",
      "[0,   533] loss: 1.208101\n",
      "[0,   534] loss: 1.287408\n",
      "[0,   535] loss: 0.978849\n",
      "[0,   536] loss: 0.971412\n",
      "[0,   537] loss: 1.013344\n",
      "[0,   538] loss: 1.584797\n",
      "[0,   539] loss: 1.117115\n",
      "[0,   540] loss: 1.088836\n",
      "[0,   541] loss: 1.176010\n",
      "[0,   542] loss: 1.131016\n",
      "[0,   543] loss: 0.885902\n",
      "[0,   544] loss: 1.456379\n",
      "[0,   545] loss: 1.341530\n",
      "[0,   546] loss: 1.146543\n",
      "[0,   547] loss: 1.102941\n",
      "[0,   548] loss: 1.333325\n",
      "[0,   549] loss: 1.269204\n",
      "[0,   550] loss: 0.975196\n",
      "[0,   551] loss: 1.328117\n",
      "[0,   552] loss: 1.120396\n",
      "[0,   553] loss: 1.236851\n",
      "[0,   554] loss: 1.366018\n",
      "[0,   555] loss: 1.148942\n",
      "[0,   556] loss: 1.463012\n",
      "[0,   557] loss: 0.938905\n",
      "[0,   558] loss: 1.053911\n",
      "[0,   559] loss: 0.810531\n",
      "[0,   560] loss: 1.132902\n",
      "[0,   561] loss: 1.053630\n",
      "[0,   562] loss: 1.476676\n",
      "[0,   563] loss: 1.106624\n",
      "[0,   564] loss: 0.988458\n",
      "[0,   565] loss: 1.293902\n",
      "[0,   566] loss: 1.392347\n",
      "[0,   567] loss: 1.362992\n",
      "[0,   568] loss: 1.202792\n",
      "[0,   569] loss: 0.905163\n",
      "[0,   570] loss: 1.021925\n",
      "[0,   571] loss: 1.287930\n",
      "[0,   572] loss: 1.265293\n",
      "[0,   573] loss: 1.484840\n",
      "[0,   574] loss: 1.212437\n",
      "[0,   575] loss: 1.323201\n",
      "[0,   576] loss: 1.172390\n",
      "[0,   577] loss: 1.119847\n",
      "[0,   578] loss: 0.894653\n",
      "[0,   579] loss: 0.918502\n",
      "[0,   580] loss: 1.252588\n",
      "[0,   581] loss: 1.161386\n",
      "[0,   582] loss: 1.463294\n",
      "[0,   583] loss: 1.020177\n",
      "[0,   584] loss: 1.138015\n",
      "[0,   585] loss: 1.151378\n",
      "[0,   586] loss: 1.151929\n",
      "[0,   587] loss: 1.013730\n",
      "[0,   588] loss: 1.134047\n",
      "[0,   589] loss: 1.357393\n",
      "[0,   590] loss: 1.100262\n",
      "[0,   591] loss: 1.161045\n",
      "[0,   592] loss: 1.144462\n",
      "[0,   593] loss: 1.002762\n",
      "[0,   594] loss: 1.128495\n",
      "[0,   595] loss: 1.176961\n",
      "[0,   596] loss: 1.130244\n",
      "[0,   597] loss: 1.157109\n",
      "[0,   598] loss: 1.252425\n",
      "[0,   599] loss: 0.972133\n",
      "[0,   600] loss: 1.392595\n",
      "[0,   601] loss: 1.090037\n",
      "[0,   602] loss: 1.507801\n",
      "[0,   603] loss: 1.292607\n",
      "[0,   604] loss: 1.199553\n",
      "[0,   605] loss: 1.354207\n",
      "[0,   606] loss: 1.066627\n",
      "[0,   607] loss: 0.963698\n",
      "[0,   608] loss: 1.205480\n",
      "[0,   609] loss: 1.270032\n",
      "[0,   610] loss: 1.147520\n",
      "[0,   611] loss: 1.644294\n",
      "[0,   612] loss: 1.270615\n",
      "[0,   613] loss: 1.416311\n",
      "[0,   614] loss: 1.019142\n",
      "[0,   615] loss: 1.213079\n",
      "[0,   616] loss: 1.298267\n",
      "[0,   617] loss: 1.278721\n",
      "[0,   618] loss: 1.261258\n",
      "[0,   619] loss: 1.148294\n",
      "[0,   620] loss: 1.031067\n",
      "[0,   621] loss: 1.256398\n",
      "[0,   622] loss: 1.018841\n",
      "[0,   623] loss: 1.023245\n",
      "[0,   624] loss: 1.161557\n",
      "[0,   625] loss: 1.059434\n",
      "[0,   626] loss: 1.037861\n",
      "[0,   627] loss: 1.382812\n",
      "[0,   628] loss: 1.418989\n",
      "[0,   629] loss: 1.022121\n",
      "[0,   630] loss: 1.222682\n",
      "[0,   631] loss: 1.362895\n",
      "[0,   632] loss: 1.149669\n",
      "[0,   633] loss: 1.162878\n",
      "[0,   634] loss: 1.278226\n",
      "[0,   635] loss: 0.863731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,   636] loss: 1.461152\n",
      "[0,   637] loss: 1.171729\n",
      "[0,   638] loss: 1.185269\n",
      "[0,   639] loss: 1.269960\n",
      "[0,   640] loss: 1.246250\n",
      "[0,   641] loss: 1.226444\n",
      "[0,   642] loss: 1.054441\n",
      "[0,   643] loss: 1.094177\n",
      "[0,   644] loss: 1.244711\n",
      "[0,   645] loss: 1.057804\n",
      "[0,   646] loss: 1.459265\n",
      "[0,   647] loss: 1.261202\n",
      "[0,   648] loss: 1.139903\n",
      "[0,   649] loss: 0.958971\n",
      "[0,   650] loss: 1.241972\n",
      "[0,   651] loss: 0.804814\n",
      "[0,   652] loss: 1.018226\n",
      "[0,   653] loss: 1.519037\n",
      "[0,   654] loss: 1.115040\n",
      "[0,   655] loss: 1.013276\n",
      "[0,   656] loss: 1.245167\n",
      "[0,   657] loss: 1.076848\n",
      "[0,   658] loss: 1.386751\n",
      "[0,   659] loss: 1.210990\n",
      "[0,   660] loss: 1.142682\n",
      "[0,   661] loss: 1.099125\n",
      "[0,   662] loss: 1.189381\n",
      "[0,   663] loss: 1.292530\n",
      "[0,   664] loss: 1.290718\n",
      "[0,   665] loss: 1.373301\n",
      "[0,   666] loss: 1.165509\n",
      "[0,   667] loss: 1.058981\n",
      "[0,   668] loss: 1.070099\n",
      "[0,   669] loss: 1.446982\n",
      "[0,   670] loss: 1.155806\n",
      "[0,   671] loss: 0.846885\n",
      "[0,   672] loss: 1.496352\n",
      "[0,   673] loss: 1.266340\n",
      "[0,   674] loss: 1.372336\n",
      "[0,   675] loss: 0.920628\n",
      "[0,   676] loss: 1.195020\n",
      "[0,   677] loss: 0.989092\n",
      "[0,   678] loss: 1.712482\n",
      "[0,   679] loss: 1.151680\n",
      "[0,   680] loss: 0.928891\n",
      "[0,   681] loss: 0.764014\n",
      "[0,   682] loss: 1.161794\n",
      "[0,   683] loss: 1.228924\n",
      "[0,   684] loss: 0.838897\n",
      "[0,   685] loss: 1.049289\n",
      "[0,   686] loss: 1.634949\n",
      "[0,   687] loss: 1.204508\n",
      "[0,   688] loss: 1.117777\n",
      "[0,   689] loss: 1.413331\n",
      "[0,   690] loss: 1.178472\n",
      "[0,   691] loss: 1.200402\n",
      "[0,   692] loss: 1.083279\n",
      "[0,   693] loss: 1.233700\n",
      "[0,   694] loss: 1.178009\n",
      "[0,   695] loss: 1.138508\n",
      "[0,   696] loss: 1.172009\n",
      "[0,   697] loss: 1.252274\n",
      "[0,   698] loss: 1.318069\n",
      "[0,   699] loss: 1.116378\n",
      "[0,   700] loss: 1.405433\n",
      "[0,   701] loss: 0.911177\n",
      "[0,   702] loss: 1.283131\n",
      "[0,   703] loss: 1.414279\n",
      "[0,   704] loss: 1.061673\n",
      "[0,   705] loss: 1.155870\n",
      "[0,   706] loss: 1.268891\n",
      "[0,   707] loss: 1.046471\n",
      "[0,   708] loss: 1.151804\n",
      "[0,   709] loss: 1.020194\n",
      "[0,   710] loss: 1.238213\n",
      "[0,   711] loss: 0.995986\n",
      "[0,   712] loss: 1.218812\n",
      "[0,   713] loss: 1.289201\n",
      "[0,   714] loss: 1.061521\n",
      "[0,   715] loss: 1.171500\n",
      "[0,   716] loss: 1.368773\n",
      "[0,   717] loss: 0.948523\n",
      "[0,   718] loss: 1.254040\n",
      "[0,   719] loss: 0.862313\n",
      "[0,   720] loss: 1.034718\n",
      "[0,   721] loss: 1.260771\n",
      "[0,   722] loss: 1.112271\n",
      "[0,   723] loss: 1.151384\n",
      "[0,   724] loss: 0.904542\n",
      "[0,   725] loss: 1.237446\n",
      "[0,   726] loss: 1.062659\n",
      "[0,   727] loss: 0.889626\n",
      "[0,   728] loss: 1.227338\n",
      "[0,   729] loss: 1.182482\n",
      "[0,   730] loss: 1.241307\n",
      "[0,   731] loss: 1.139787\n",
      "[0,   732] loss: 1.171239\n",
      "[0,   733] loss: 1.040635\n",
      "[0,   734] loss: 1.028891\n",
      "[0,   735] loss: 1.226675\n",
      "[0,   736] loss: 1.514089\n",
      "[0,   737] loss: 1.317731\n",
      "[0,   738] loss: 1.275278\n",
      "[0,   739] loss: 1.272779\n",
      "[0,   740] loss: 1.133186\n",
      "[0,   741] loss: 1.268776\n",
      "[0,   742] loss: 1.099931\n",
      "[0,   743] loss: 1.219736\n",
      "[0,   744] loss: 1.036311\n",
      "[0,   745] loss: 1.118517\n",
      "[0,   746] loss: 0.961792\n",
      "[0,   747] loss: 1.073589\n",
      "[0,   748] loss: 1.105840\n",
      "[0,   749] loss: 1.191802\n",
      "[0,   750] loss: 1.427734\n",
      "[0,   751] loss: 1.287940\n",
      "[0,   752] loss: 1.069139\n",
      "[0,   753] loss: 1.160521\n",
      "[0,   754] loss: 1.046098\n",
      "[0,   755] loss: 1.492284\n",
      "[0,   756] loss: 1.236253\n",
      "[0,   757] loss: 0.896610\n",
      "[0,   758] loss: 1.102895\n",
      "[0,   759] loss: 1.455112\n",
      "[0,   760] loss: 1.257543\n",
      "[0,   761] loss: 1.150847\n",
      "[0,   762] loss: 1.128609\n",
      "[0,   763] loss: 1.256178\n",
      "[0,   764] loss: 1.183916\n",
      "[0,   765] loss: 1.207058\n",
      "[0,   766] loss: 1.311313\n",
      "[0,   767] loss: 1.338718\n",
      "[0,   768] loss: 1.034867\n",
      "[0,   769] loss: 0.967193\n",
      "[0,   770] loss: 1.091983\n",
      "[0,   771] loss: 1.167674\n",
      "[0,   772] loss: 1.219039\n",
      "[0,   773] loss: 0.875603\n",
      "[0,   774] loss: 1.174388\n",
      "[0,   775] loss: 1.001370\n",
      "[0,   776] loss: 1.047095\n",
      "[0,   777] loss: 1.484560\n",
      "[0,   778] loss: 1.191100\n",
      "[0,   779] loss: 1.340743\n",
      "[0,   780] loss: 1.080228\n",
      "[0,   781] loss: 1.058046\n",
      "[0,   782] loss: 1.165460\n",
      "[0,   783] loss: 1.249691\n",
      "[0,   784] loss: 1.380481\n",
      "[0,   785] loss: 0.998397\n",
      "[0,   786] loss: 1.205276\n",
      "[0,   787] loss: 1.324038\n",
      "[0,   788] loss: 1.269187\n",
      "[0,   789] loss: 1.333636\n",
      "[0,   790] loss: 1.192806\n",
      "[0,   791] loss: 1.181012\n",
      "[0,   792] loss: 1.001101\n",
      "[0,   793] loss: 1.231788\n",
      "[0,   794] loss: 1.013270\n",
      "[0,   795] loss: 1.193658\n",
      "[0,   796] loss: 1.066072\n",
      "[0,   797] loss: 0.875973\n",
      "[0,   798] loss: 1.196847\n",
      "[0,   799] loss: 0.997921\n",
      "[0,   800] loss: 1.346949\n",
      "[0,   801] loss: 1.212786\n",
      "[0,   802] loss: 1.356417\n",
      "[0,   803] loss: 1.204820\n",
      "[0,   804] loss: 0.950697\n",
      "[0,   805] loss: 0.870128\n",
      "[0,   806] loss: 1.073307\n",
      "[0,   807] loss: 1.059732\n",
      "[0,   808] loss: 1.090750\n",
      "[0,   809] loss: 1.183785\n",
      "[0,   810] loss: 1.134965\n",
      "[0,   811] loss: 1.059985\n",
      "[0,   812] loss: 1.034427\n",
      "[0,   813] loss: 1.043909\n",
      "[0,   814] loss: 1.202372\n",
      "[0,   815] loss: 1.345614\n",
      "[0,   816] loss: 1.368840\n",
      "[0,   817] loss: 1.347687\n",
      "[0,   818] loss: 1.221246\n",
      "[0,   819] loss: 1.384458\n",
      "[0,   820] loss: 1.090114\n",
      "[0,   821] loss: 1.402113\n",
      "[0,   822] loss: 1.333971\n",
      "[0,   823] loss: 1.502065\n",
      "[0,   824] loss: 1.141008\n",
      "[0,   825] loss: 1.268937\n",
      "[0,   826] loss: 1.276488\n",
      "[0,   827] loss: 0.958868\n",
      "[0,   828] loss: 0.913801\n",
      "[0,   829] loss: 1.026982\n",
      "[0,   830] loss: 1.293147\n",
      "[0,   831] loss: 1.144099\n",
      "[0,   832] loss: 1.554883\n",
      "[0,   833] loss: 1.262684\n",
      "[0,   834] loss: 0.993467\n",
      "[0,   835] loss: 1.140218\n",
      "[0,   836] loss: 1.106504\n",
      "[0,   837] loss: 1.200795\n",
      "[0,   838] loss: 1.104065\n",
      "[0,   839] loss: 1.119690\n",
      "[0,   840] loss: 1.182642\n",
      "[0,   841] loss: 1.091121\n",
      "[0,   842] loss: 1.095129\n",
      "[0,   843] loss: 1.266496\n",
      "[0,   844] loss: 1.098613\n",
      "[0,   845] loss: 1.104017\n",
      "[0,   846] loss: 1.533919\n",
      "[0,   847] loss: 0.894727\n",
      "[0,   848] loss: 1.238772\n",
      "[0,   849] loss: 0.914194\n",
      "[0,   850] loss: 1.216346\n",
      "[0,   851] loss: 1.103216\n",
      "[0,   852] loss: 1.387834\n",
      "[0,   853] loss: 1.080039\n",
      "[0,   854] loss: 1.144597\n",
      "[0,   855] loss: 0.995020\n",
      "[0,   856] loss: 0.927988\n",
      "[0,   857] loss: 1.037621\n",
      "[0,   858] loss: 0.894071\n",
      "[0,   859] loss: 0.746936\n",
      "[0,   860] loss: 1.161209\n",
      "[0,   861] loss: 0.881002\n",
      "[0,   862] loss: 1.072570\n",
      "[0,   863] loss: 0.884457\n",
      "[0,   864] loss: 1.319038\n",
      "[0,   865] loss: 1.540780\n",
      "[0,   866] loss: 0.973926\n",
      "[0,   867] loss: 0.978057\n",
      "[0,   868] loss: 1.307703\n",
      "[0,   869] loss: 1.289234\n",
      "[0,   870] loss: 1.007151\n",
      "[0,   871] loss: 1.161233\n",
      "[0,   872] loss: 1.218105\n",
      "[0,   873] loss: 1.082638\n",
      "[0,   874] loss: 1.346155\n",
      "[0,   875] loss: 1.260078\n",
      "[0,   876] loss: 1.209726\n",
      "[0,   877] loss: 1.229208\n",
      "[0,   878] loss: 1.222326\n",
      "[0,   879] loss: 1.390556\n",
      "[0,   880] loss: 1.334708\n",
      "[0,   881] loss: 1.262269\n",
      "[0,   882] loss: 1.183626\n",
      "[0,   883] loss: 1.084195\n",
      "[0,   884] loss: 1.312113\n",
      "[0,   885] loss: 0.954541\n",
      "[0,   886] loss: 1.116924\n",
      "[0,   887] loss: 1.292448\n",
      "[0,   888] loss: 1.332222\n",
      "[0,   889] loss: 1.272067\n",
      "[0,   890] loss: 1.186100\n",
      "[0,   891] loss: 1.236612\n",
      "[0,   892] loss: 1.196171\n",
      "[0,   893] loss: 1.086137\n",
      "[0,   894] loss: 1.416832\n",
      "[0,   895] loss: 0.941111\n",
      "[0,   896] loss: 1.059406\n",
      "[0,   897] loss: 1.231989\n",
      "[0,   898] loss: 1.188683\n",
      "[0,   899] loss: 1.058159\n",
      "[0,   900] loss: 1.226961\n",
      "[0,   901] loss: 1.320397\n",
      "[0,   902] loss: 1.240221\n",
      "[0,   903] loss: 1.197514\n",
      "[0,   904] loss: 1.264692\n",
      "[0,   905] loss: 0.862829\n",
      "[0,   906] loss: 0.993468\n",
      "[0,   907] loss: 1.274382\n",
      "[0,   908] loss: 1.425854\n",
      "[0,   909] loss: 1.361595\n",
      "[0,   910] loss: 1.116949\n",
      "[0,   911] loss: 0.956300\n",
      "[0,   912] loss: 1.456664\n",
      "[0,   913] loss: 1.327690\n",
      "[0,   914] loss: 1.069910\n",
      "[0,   915] loss: 1.233339\n",
      "[0,   916] loss: 1.287785\n",
      "[0,   917] loss: 0.992907\n",
      "[0,   918] loss: 1.321647\n",
      "[0,   919] loss: 0.939089\n",
      "[0,   920] loss: 1.424799\n",
      "[0,   921] loss: 1.192978\n",
      "[0,   922] loss: 1.349337\n",
      "[0,   923] loss: 1.182569\n",
      "[0,   924] loss: 1.056214\n",
      "[0,   925] loss: 1.038375\n",
      "[0,   926] loss: 1.145929\n",
      "[0,   927] loss: 1.222181\n",
      "[0,   928] loss: 1.220524\n",
      "[0,   929] loss: 1.348118\n",
      "[0,   930] loss: 1.193781\n",
      "[0,   931] loss: 1.274955\n",
      "[0,   932] loss: 1.126374\n",
      "[0,   933] loss: 1.212325\n",
      "[0,   934] loss: 0.989491\n",
      "[0,   935] loss: 1.398837\n",
      "[0,   936] loss: 1.318241\n",
      "[0,   937] loss: 1.218211\n",
      "[0,   938] loss: 0.978134\n",
      "[0,   939] loss: 1.119544\n",
      "[0,   940] loss: 1.394397\n",
      "[0,   941] loss: 1.322000\n",
      "[0,   942] loss: 1.185509\n",
      "[0,   943] loss: 1.349860\n",
      "[0,   944] loss: 1.091450\n",
      "[0,   945] loss: 1.080245\n",
      "[0,   946] loss: 1.217822\n",
      "[0,   947] loss: 1.167343\n",
      "[0,   948] loss: 0.844803\n",
      "[0,   949] loss: 1.114834\n",
      "[0,   950] loss: 1.275789\n",
      "[0,   951] loss: 1.263126\n",
      "[0,   952] loss: 1.105748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,   953] loss: 1.072289\n",
      "[0,   954] loss: 1.182936\n",
      "[0,   955] loss: 1.074536\n",
      "[0,   956] loss: 1.006317\n",
      "[0,   957] loss: 0.957295\n",
      "[0,   958] loss: 1.155344\n",
      "[0,   959] loss: 1.046996\n",
      "[0,   960] loss: 1.029262\n",
      "[0,   961] loss: 1.261443\n",
      "[0,   962] loss: 1.076623\n",
      "[0,   963] loss: 0.931999\n",
      "[0,   964] loss: 1.242654\n",
      "[0,   965] loss: 1.228850\n",
      "[0,   966] loss: 0.971838\n",
      "[0,   967] loss: 1.011822\n",
      "[0,   968] loss: 1.185389\n",
      "[0,   969] loss: 1.313441\n",
      "[0,   970] loss: 1.280152\n",
      "[0,   971] loss: 1.194435\n",
      "[0,   972] loss: 1.421664\n",
      "[0,   973] loss: 0.991534\n",
      "[0,   974] loss: 1.032630\n",
      "[0,   975] loss: 1.226619\n",
      "[0,   976] loss: 1.262743\n",
      "[0,   977] loss: 1.256902\n",
      "[0,   978] loss: 1.140178\n",
      "[0,   979] loss: 1.248571\n",
      "[0,   980] loss: 1.107079\n",
      "[0,   981] loss: 1.177447\n",
      "[0,   982] loss: 1.182308\n",
      "[0,   983] loss: 1.062226\n",
      "[0,   984] loss: 1.190386\n",
      "[0,   985] loss: 1.033884\n",
      "[0,   986] loss: 0.995080\n",
      "[0,   987] loss: 1.098841\n",
      "[0,   988] loss: 1.137057\n",
      "[0,   989] loss: 1.181003\n",
      "[0,   990] loss: 0.855409\n",
      "[0,   991] loss: 0.958065\n",
      "[0,   992] loss: 1.127245\n",
      "[0,   993] loss: 1.120327\n",
      "[0,   994] loss: 1.139540\n",
      "[0,   995] loss: 1.009554\n",
      "[0,   996] loss: 1.134903\n",
      "[0,   997] loss: 1.052837\n",
      "[0,   998] loss: 1.234215\n",
      "[0,   999] loss: 1.237650\n"
     ]
    }
   ],
   "source": [
    "# Обучаем нейронную сеть\n",
    "\n",
    "MAX_EPOCH = 1                                             # Максимальное количество проходов по dataset'y\n",
    "\n",
    "for epoch in range( MAX_EPOCH ):\n",
    "    for step, ( b_images, b_labels ) in enumerate( train_loader ):    # Для всех изображений b_images и разметок b_labels \n",
    "        output = cnn( b_images )                                      # Записываем результат работы сети в output\n",
    "        loss   = loss_func( output, b_labels )                        # Считаем ошибку между результатом output и заданной \n",
    "                                                                      # разметкой b_labels\n",
    "        optimizer.zero_grad()                                         # Обнуляем градиент\n",
    "        loss.backward()                                               # Используем метод обратного распространения ошибки\n",
    "        optimizer.step()                                              # Применяем градиент\n",
    "        \n",
    "        running_loss = loss.item()                                    # Получаем ошибку на текущем шаге\n",
    "        print( '[%d, %5d] loss: %f' % ( epoch, step, running_loss ) ) # Выводим ошибку на экран\n",
    "        running_loss = 0                                              # Обнуляем ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Загружаем dataset для тестирования\n",
    "\n",
    "# Объявляем необходимую для визуализаци функцию трансформации\n",
    "transform = transforms.Compose( [ \n",
    "        transforms.ToTensor()\n",
    "      , transforms.Normalize( ( 0.5,0.5, 0.5 ), ( 0.5, 0.5, 0.5 ) ) \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Скачиваем данные для тестирования работы нейронной сети по аналогии с train_data\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "      root = './cifar10/'\n",
    "    , train     = False\n",
    "    , download  = True\n",
    "    , transform = transform\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "      test_data\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , shuffle    = True\n",
    "    , num_workers = 2                      # Сколько процессов использоват для загрзки данных\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пишем вспомогательную функцию для визуализаци.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow( img ):\n",
    "    img   = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow( np.transpose( npimg, (1, 2, 0) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  dog dog deer automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c2663a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Объявляем вектор, содержащий названия всех классов изображений\n",
    "classes   = ( 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck' )\n",
    "# Объявляем итератор по всему тестовому dataset'y\n",
    "data_iter = iter( test_loader )\n",
    "# Считываем изображение и разметку.\n",
    "images, labels = data_iter.next()\n",
    "# Выводим изображение на экран\n",
    "imshow( torchvision.utils.make_grid( images ) ) \n",
    "# Выводим соответствующие классы\n",
    "print( 'GroundTruth: ', ' '.join( '%s' % classes[labels[j]] for j in range( BATCH_SIZE ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  truck frog deer automobile\n"
     ]
    }
   ],
   "source": [
    "# Запускаем нейросеть на первых BATCH_SIZE изображениях из тестовой выборки\n",
    "outputs = cnn( images )\n",
    "_, predicted = torch.max( outputs, 1 )\n",
    "print( \"Predicted: \", ' '.join( '%s' % classes[predicted[j]] for j in range ( BATCH_SIZE ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишем функцию оценки точности работы получившейся сети\n",
    "\n",
    "class_correct = list( 0. for i in range(10) )                     # Выделим нулевой вектор размера 10 (количество классов)\n",
    "                                                                  # для весов корректно распознанных классов\n",
    "class_total   = list( 0. for i in range(10) )                     # -- || -- для общих весов классов\n",
    "with torch.no_grad():                                             # Указываем на то, что градиент не используется\n",
    "    for data in test_loader:                                      # Для всех изображений тестового dataset'a\n",
    "        images, labels = data                                     # Получаем текущие изображение и разметку\n",
    "        outputs = cnn( images )                                   # Применяем нейронную сеть к изображению\n",
    "        _, predicted = torch.max( outputs, 1 )                    \n",
    "        c = ( predicted == labels ).squeeze()\n",
    "        for i in range( BATCH_SIZE ):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label]   += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of airplane : 47 %\n",
      "Accuracy of automobile : 91 %\n",
      "Accuracy of bird : 26 %\n",
      "Accuracy of cat : 48 %\n",
      "Accuracy of deer : 7 %\n",
      "Accuracy of dog : 26 %\n",
      "Accuracy of frog : 70 %\n",
      "Accuracy of horse : 50 %\n",
      "Accuracy of ship : 21 %\n",
      "Accuracy of truck : 57 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print( 'Accuracy of %s : %d %%' % (classes[i], 100 * class_correct[i] / class_total[i] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
